{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e32f01a-32fd-41f0-b5d8-d2a83d7a6a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp datadownload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1f2596-35b6-4e34-8c91-4179484884b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import csv\n",
    "from io import StringIO\n",
    "from rich import inspect\n",
    "import pandas as pd\n",
    "import json\n",
    "import duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d648c9-d9ac-441a-83e3-a579a28b9fa3",
   "metadata": {},
   "source": [
    "# Data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c323f3-ac14-4e12-922a-644680f56b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def extract_metadata(metadata_url):\n",
    "    \"\"\"\n",
    "    Extracts a list of filenames and urls from Open Cananda metadata url.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    metadata_url : str\n",
    "        Fuel consumption ratings metadata url from Open Canada website.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    english_resources_df : pd.DataFrame\n",
    "        DataFrame of file names and urls for energy consumption ratings.\n",
    "    \"\"\"\n",
    "    try:      \n",
    "        metadata_resp = requests.get(metadata_url)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # If request fails, return an error message and stop.\n",
    "        print(f'Error making url request: {e}')\n",
    "    \n",
    "    try:     \n",
    "        metadata_json = metadata_resp.json()\n",
    "    except json.JSONDecodeError:\n",
    "        # If parsing json fails, return an error message and stop.\n",
    "        print(f'Error: Response is not valid json')\n",
    "        \n",
    "    # Access list of downloadable resources\n",
    "    resources_df = pd.DataFrame(metadata_json['result']['resources'])\n",
    "\n",
    "    # Change language coding and extract English only resources\n",
    "    resources_df['language'] = resources_df['language'].apply(lambda item : item[0])\n",
    "    english_resources_df = resources_df[resources_df['language'] == 'en']\n",
    "    \n",
    "    return english_resources_df[['name', 'url']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e7840a-cb32-471a-ad23-256aab1d1d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def extract_raw_data(url, file_name):\n",
    "    \"\"\"\n",
    "    Extract raw data from a URL\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        URL to extract data from\n",
    "        \n",
    "    file_name : str or Path object\n",
    "        file name for raw data dump\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Request data from url\n",
    "        response = requests.get(url)\n",
    "        content_type = response.headers['content-type']\n",
    "        response_text = response.text\n",
    "        print(f'Response status: {response.status_code}\\nContent Type: {content_type}')\n",
    "\n",
    "        # Save request content to csv file\n",
    "        with open(file_name, mode='w', newline='') as csvfile:\n",
    "            csvfile.write(response_text)\n",
    "\n",
    "        print(f'csv file: {file_name} saved')\n",
    "        \n",
    "    # Catch errors    \n",
    "    except requests.exceptions.HTTPError as err_h:\n",
    "        print(f'HTTP error occured:{err_h}')\n",
    "    except requests.exceptions.ConnectionError as err_c:\n",
    "        print(f'Error connecting:{err_c}')\n",
    "    except requests.exceptions.Timeout as err_t:\n",
    "        print(f'Timeout Error:{err_t}')\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        print(f'There was an unknown error:{err}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e4db12-45e6-4c38-a96e-edf9c90213ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def merge_top_two_rows(input_file, output_file):\n",
    "    # Open the input CSV file for reading\n",
    "    with open(input_file, mode='r', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        \n",
    "        # Read the first two rows from the input file\n",
    "        header_row = next(reader)\n",
    "        second_row = next(reader)\n",
    "        \n",
    "        # Merge the two rows into one header\n",
    "        merged_header = [f\"{header_row[i]} {second_row[i]}\" for i in range(len(header_row))]\n",
    "        \n",
    "        # Open the output CSV file for writing\n",
    "        with open(output_file, mode='w', newline='') as output_csvfile:\n",
    "            writer = csv.writer(output_csvfile)\n",
    "            \n",
    "            # Write the merged header to the output file\n",
    "            writer.writerow(merged_header)\n",
    "            \n",
    "            # Copy the rest of the rows from the input file to the output file\n",
    "            for row in reader:\n",
    "                writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e0a707-c502-4f24-a54a-9dfcb5ae41dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def rename_columns(df):\n",
    "    \"\"\"\n",
    "    Removes unwanted DataFrame columns and rows, then cleans and renames column headers.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: DataFrame\n",
    "        DataFrame with columns to clean\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df: DataFrame\n",
    "        DataFrame with cleaned column headers\n",
    "\n",
    "    \"\"\"\n",
    "    # Drop empty columns and rows from the DataFrame\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "    df.dropna(axis=0, thresh=5, inplace=True)\n",
    "\n",
    "    # Remove whitespace, replace spaces with _ and change to lower case\n",
    "    cleaned_cols = (df.columns.str.lower()\n",
    "                    .str.strip()\n",
    "                    .str.replace(' # = high output engine', '')\n",
    "                    .str.replace('*', '')\n",
    "                    .str.replace('  ', ' ')\n",
    "                    .str.replace(' ', '_')\n",
    "                    .str.replace('(', '')\n",
    "                    .str.replace(')', '')\n",
    "                    .str.replace('/', '_')\n",
    "    )\n",
    "\n",
    "    col_mapper = dict(list(zip(df.columns, cleaned_cols))) # build a dictionary to map old column names to new\n",
    "    df.rename(columns=col_mapper, inplace=True)\n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4b526a-7c1d-4b9f-bec2-0d6c4e8006bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def clean_content(df):\n",
    "    \"\"\"\n",
    "    Clean content of master_df columns\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Should be master_df containing all fuel rating data combined\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set fuel type columns using fuel_dict\n",
    "    df['fuel_type'] = df['fuel_type'].map(fuel_dict)\n",
    "    df['fuel_type_1'] = df['fuel_type_1'].map(fuel_dict)\n",
    "    df['fuel_type_2'] = df['fuel_type_2'].map(fuel_dict)\n",
    "\n",
    "    # Set make, model and vehicle_class to lower case and remove \":\" characters\n",
    "    df['make'] = df['make'].str.lower().str.strip()\n",
    "    df['model'] = df['model'].str.lower().str.strip()\n",
    "    df['vehicle_class'] = df['vehicle_class'].str.lower().str.strip()\n",
    "    df['vehicle_class'] = df['vehicle_class'].str.replace(\":\", \"-\")\n",
    "\n",
    "    # Set make, model and vehicle_class to category columns\n",
    "    df['make'] = df['make'].astype('category')\n",
    "    df['model'] = df['model'].astype('category')\n",
    "    df['vehicle_class'] = df['vehicle_class'].astype('category')\n",
    "\n",
    "    # Split transmission column into transmission type and number of gears\n",
    "    df = df.join(df['transmission'].str.split(r\"(\\d+)\", expand=True)\n",
    "                          .drop(columns=[2])\n",
    "                          .rename(columns={0: 'transmission_type', 1: 'number_of_gears'})\n",
    "    )\n",
    "\n",
    "    df['transmission_type'] = df['transmission_type'].map(transmission_dict)\n",
    "    df.drop(columns='transmission', inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8babc4-d2ff-47ec-b2d4-5a2c3922a94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def init_duckdb(db_file_path, tables):\n",
    "    \"\"\"\n",
    "    Initialize a DuckDB data base and create tables for each DataFrame\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    db_file_path : str\n",
    "        Path to the DuckDB database file\n",
    "    tables : list\n",
    "        Dictionary of table names and references to DataFrames for those tables \n",
    "    \"\"\"\n",
    "    db_connection = duckdb.connect(db_file_path)\n",
    "    for key, value in tables.items():\n",
    "        create_duckdb_table(db_connection, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49e8837-ed92-4c60-9ef1-00f7b3edb0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def create_duckdb_table(db_connection, table_name, df):\n",
    "    \"\"\"\n",
    "    Create a table in DuckDB\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    db_connection : duckdb.conect\n",
    "        Connection to DuckDB\n",
    "    table_name : str\n",
    "        Name of the table to be created\n",
    "    df : str\n",
    "        Nmae of the DataFrame to be used to create the table.\n",
    "    \"\"\"\n",
    "    db_connection.execute(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "    db_connection.execute(f\"CREATE TABLE {table_name} AS SELECT * FROM {df}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23d0b25-16cc-4579-bcc7-70d62aaec41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "global model_dict\n",
    "global transmission_dict\n",
    "global fuel_dict\n",
    "\n",
    "model_dict = {\n",
    "    \"4wd/4X4\": \"Four-wheel drive\",\n",
    "    \"awd\": \"All-wheel drive\",\n",
    "    \"ffv\": \"Flexible-fuel vehicle\",\n",
    "    \"swb\": \"Short wheelbase\",\n",
    "    \"lwb\": \"Long wheelbase\",\n",
    "    \"ewb\": \"Extended wheelbase\",\n",
    "    \"cng\": \"Compressed natural gas\",\n",
    "    \"ngv\": \"Natural gas vehicle\",\n",
    "    \"#\": \"High output engine that \\\n",
    "            provides more power than the standard \\\n",
    "            engine of the same size\",\n",
    "}\n",
    "\n",
    "transmission_dict = {\n",
    "    \"A\": \"automatic\",\n",
    "    \"AM\": \"automated manual\",\n",
    "    \"AS\": \"automatic with select Shift\",\n",
    "    \"AV\": \"continuously variable\",\n",
    "    \"M\": \"manual\",\n",
    "}\n",
    "\n",
    "fuel_dict = {\n",
    "    \"X\": \"regular gasoline\",\n",
    "    \"Z\": \"premium gasoline\",\n",
    "    \"D\": \"diesel\",\n",
    "    \"E\": \"ethanol (E85)\",\n",
    "    \"N\": \"natural gas\",\n",
    "    \"B\": \"electricity\",\n",
    "    \"B/X\": \"electricity & regular gasoline\",\n",
    "    \"B/Z\": \"electricity & premium gasoline\",\n",
    "    \"B/Z*\": \"electricity & premium gasoline\",\n",
    "    \"B/X*\": \"electricity & regular gasoline\",\n",
    "    \"B\": \"electricity\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e801ec-e08b-4573-958d-4e30eb578101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response status: 200\n",
      "Content Type: text/csv\n",
      "csv file: /home/jsh/vehicle-co2/data/raw/battery-electric_vehicles_2012-2023_v_2023-08-18.csv saved\n",
      "Response status: 200\n",
      "Content Type: text/csv\n",
      "csv file: /home/jsh/vehicle-co2/data/raw/plug-in_hybrid_electric_vehicles_2012-2023_v_2023-08-18.csv saved\n",
      "Response status: 200\n",
      "Content Type: text/csv\n",
      "csv file: /home/jsh/vehicle-co2/data/raw/2023_fuel_consumption_ratings_v_2023-08-18.csv saved\n",
      "Response status: 200\n",
      "Content Type: text/csv\n",
      "csv file: /home/jsh/vehicle-co2/data/raw/2022_fuel_consumption_ratings_v_2023-08-18.csv saved\n",
      "Response status: 200\n",
      "Content Type: text/csv\n",
      "csv file: /home/jsh/vehicle-co2/data/raw/2021_fuel_consumption_ratings_v_2023-02-03.csv saved\n",
      "Response status: 200\n",
      "Content Type: text/csv\n",
      "csv file: /home/jsh/vehicle-co2/data/raw/2020_fuel_consumption_ratings_v_2023-02-03.csv saved\n",
      "Response status: 200\n",
      "Content Type: text/csv\n",
      "csv file: /home/jsh/vehicle-co2/data/raw/2019_fuel_consumption_ratings_v_2021-09-29.csv saved\n",
      "Response status: 200\n",
      "Content Type: text/csv\n",
      "csv file: /home/jsh/vehicle-co2/data/raw/2018_fuel_consumption_ratings_v_2021-09-29.csv saved\n",
      "Response status: 200\n",
      "Content Type: text/csv\n",
      "csv file: /home/jsh/vehicle-co2/data/raw/2017_fuel_consumption_ratings_v_2020-03-17.csv saved\n",
      "Response status: 200\n",
      "Content Type: text/csv\n",
      "csv file: /home/jsh/vehicle-co2/data/raw/2016_fuel_consumption_ratings_v_2020-03-17.csv saved\n",
      "Response status: 200\n",
      "Content Type: text/csv\n",
      "csv file: /home/jsh/vehicle-co2/data/raw/2015_fuel_consumption_ratings_v_2020-03-17.csv saved\n",
      "Response status: 200\n",
      "Content Type: text/csv\n",
      "csv file: /home/jsh/vehicle-co2/data/raw/2010-2014_fuel_consumption_ratings_v_2020-03-17.csv saved\n",
      "Response status: 200\n",
      "Content Type: text/csv\n",
      "csv file: /home/jsh/vehicle-co2/data/raw/2005-2009_fuel_consumption_ratings_v_2020-01-31.csv saved\n",
      "Response status: 200\n",
      "Content Type: text/csv\n",
      "csv file: /home/jsh/vehicle-co2/data/raw/2000-2004_fuel_consumption_ratings.csv saved\n",
      "Response status: 200\n",
      "Content Type: text/csv\n",
      "csv file: /home/jsh/vehicle-co2/data/raw/1995-1999_fuel_consumption_ratings.csv saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2989/1533119161.py:47: DtypeWarning: Columns (0,1,2,3,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(merged_header_file_name)\n",
      "/tmp/ipykernel_2989/1533119161.py:47: DtypeWarning: Columns (0,1,2,3,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(merged_header_file_name)\n",
      "/tmp/ipykernel_2989/1533119161.py:47: DtypeWarning: Columns (0,1,2,3,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(merged_header_file_name)\n",
      "/tmp/ipykernel_2989/1533119161.py:47: DtypeWarning: Columns (0,1,2,3,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(merged_header_file_name)\n",
      "/tmp/ipykernel_2989/1533119161.py:47: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(merged_header_file_name)\n"
     ]
    }
   ],
   "source": [
    "#|export\n",
    "if __name__ == \"__main__\":\n",
    "    url = 'https://natural-resources.canada.ca/sites/nrcan/files/oee/files/csv/MY2023%20Fuel%20Consumption%20Ratings.csv'\n",
    "    metadata_url = 'https://open.canada.ca/data/api/action/package_show?id=98f1a129-f628-4ce4-b24d-6f16bf24dd64'\n",
    "    \n",
    "    # Build list of available resources\n",
    "    resources_df = extract_metadata(metadata_url)\n",
    "    \n",
    "    # Remove unwanted old resources\n",
    "    resources_df = resources_df[~resources_df['name'].str.contains('Original')]\n",
    "    \n",
    "    # Build filenames for desired resources and add to resources_df\n",
    "    file_names = (resources_df['name']\n",
    "         .str.replace(' ', '_')\n",
    "         .str.replace('(', 'v_')\n",
    "         .str.replace(')', '')\n",
    "         .str.lower()\n",
    "    )\n",
    "    resources_df.loc[:,'file_name'] = file_names\n",
    "    \n",
    "    # Build raw data file path\n",
    "    path = Path.cwd()\n",
    "    raw_path = path.parent / 'data' / 'raw'\n",
    "    merged_header_path = path.parent / 'data' / 'merged-headers'\n",
    "    \n",
    "    # Download and save raw data for each resource\n",
    "    for idx, row in resources_df.iterrows():\n",
    "        url = row[1]\n",
    "        file_name = row[2]\n",
    "        raw_file_name = raw_path / f'{file_name}.csv'\n",
    "        merged_header_file_name = merged_header_path / f'{file_name}.csv'\n",
    "        extract_raw_data(url, raw_file_name)\n",
    "        merge_top_two_rows(raw_file_name, merged_header_file_name)\n",
    "    \n",
    "    # Start a list of column headers and initiate a master_df\n",
    "    union_of_headers = set()\n",
    "    master_df = pd.DataFrame()\n",
    "    \n",
    "    # Build the master DataFrame\n",
    "    for idx, row in resources_df.iterrows():\n",
    "        # Open each csv file\n",
    "        url = row[1]\n",
    "        file_name = row[2]\n",
    "        merged_header_file_name = merged_header_path / f'{file_name}.csv'\n",
    "    \n",
    "        # Rename the columns\n",
    "        df = pd.read_csv(merged_header_file_name)\n",
    "        df = rename_columns(df)\n",
    "    \n",
    "        # Add vehicle type based on file_name\n",
    "        if 'hybrid' in file_name:\n",
    "            df['vehicle_type'] = 'hybrid'\n",
    "        elif 'electric' in file_name and 'hybrid' not in file_name:\n",
    "            df['vehicle_type'] = 'electric'\n",
    "        else:\n",
    "            df['vehicle_type'] = 'fuel-only'\n",
    "            \n",
    "        # Add any missing column headers to master DataFrame columns\n",
    "        union_of_headers = set.union(union_of_headers, set(df.columns))\n",
    "        missing_cols = set(master_df.columns) - union_of_headers\n",
    "        if len(missing_cols) > 0:\n",
    "            for col in missing_cols:\n",
    "                master_df[col] = pd.Series()\n",
    "            \n",
    "        # Concatenate current df with master_df\n",
    "        master_df = pd.concat([master_df, df], ignore_index=True)\n",
    "        \n",
    "    # Clean the master_df\n",
    "    master_df = clean_content(master_df)\n",
    "    \n",
    "    # Create separate fuel, electric, and hybrid DataFrames\n",
    "    electric_df = master_df.loc[master_df['vehicle_type'] == 'electric'].dropna(axis=1, how='all').reset_index()\n",
    "    hybrid_df = master_df.loc[master_df['vehicle_type'] == 'hybrid'].dropna(axis=1, how='all').reset_index()\n",
    "    fuel_df = master_df.loc[master_df['vehicle_type'] == 'fuel-only'].dropna(axis=1, how='all').reset_index()\n",
    "    \n",
    "    # Create dictionary to pass to init database\n",
    "    tables = {'all_vehicles' : 'master_df', 'electric' : 'electric_df', 'hybrid' : 'hybrid_df', 'fuel' : 'fuel_df'}\n",
    "    \n",
    "    # Create directory for DuckDB database \n",
    "    db_path = path / 'data' / 'database'\n",
    "    Path(db_path).mkdir(parents=True, exist_ok=True)\n",
    "    # Create file path for DuckDB database\n",
    "    db_file_path = str(db_path / 'car_data.duckdb')\n",
    "\n",
    "    # Create DuckDB database\n",
    "    init_duckdb(db_file_path, tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7de96a-85e6-4440-b09e-7c577d67c3f0",
   "metadata": {},
   "source": [
    "## Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2e609a-4349-4a68-b476-a7f030fe60ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export notebook to module.\n",
    "from nbdev.export import nb_export\n",
    "path = Path.cwd()\n",
    "lib_path = path\n",
    "nb_path = path / 'datadownload.ipynb'\n",
    "nb_export(nb_path, lib_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pl",
   "language": "python",
   "name": "pl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
