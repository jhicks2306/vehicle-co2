{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88da6a7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T19:08:07.415788Z",
     "iopub.status.busy": "2023-10-18T19:08:07.415424Z",
     "iopub.status.idle": "2023-10-18T19:08:07.425114Z",
     "shell.execute_reply": "2023-10-18T19:08:07.423768Z"
    },
    "lines_to_next_cell": 2,
    "papermill": {
     "duration": 0.017623,
     "end_time": "2023-10-18T19:08:07.426789",
     "exception": false,
     "start_time": "2023-10-18T19:08:07.409166",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# declare a list tasks whose products you want to use as inputs\n",
    "upstream = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74970c47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T19:08:07.433248Z",
     "iopub.status.busy": "2023-10-18T19:08:07.432899Z",
     "iopub.status.idle": "2023-10-18T19:08:07.438154Z",
     "shell.execute_reply": "2023-10-18T19:08:07.436817Z"
    },
    "papermill": {
     "duration": 0.010686,
     "end_time": "2023-10-18T19:08:07.439877",
     "exception": false,
     "start_time": "2023-10-18T19:08:07.429191",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "product = {\"nb\": \"/home/jsh/vehicle-co2/pipeline/products/datadownload.ipynb\", \"data\": \"/home/jsh/vehicle-co2/pipeline/data/database/car_data.duckdb\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2176cceb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T19:08:07.445789Z",
     "iopub.status.busy": "2023-10-18T19:08:07.445544Z",
     "iopub.status.idle": "2023-10-18T19:08:07.450148Z",
     "shell.execute_reply": "2023-10-18T19:08:07.448928Z"
    },
    "papermill": {
     "duration": 0.009743,
     "end_time": "2023-10-18T19:08:07.451648",
     "exception": false,
     "start_time": "2023-10-18T19:08:07.441905",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# AUTOGENERATED! DO NOT EDIT! File to edit: datadownload.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8012f344",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T19:08:07.459127Z",
     "iopub.status.busy": "2023-10-18T19:08:07.458757Z",
     "iopub.status.idle": "2023-10-18T19:08:07.465272Z",
     "shell.execute_reply": "2023-10-18T19:08:07.464020Z"
    },
    "papermill": {
     "duration": 0.013067,
     "end_time": "2023-10-18T19:08:07.466687",
     "exception": false,
     "start_time": "2023-10-18T19:08:07.453620",
     "status": "completed"
    },
    "tags": [],
    "title": "auto 0"
   },
   "outputs": [],
   "source": [
    "__all__ = ['model_dict', 'transmission_dict', 'fuel_dict', 'extract_metadata', 'extract_raw_data', 'merge_top_two_rows',\n",
    "           'rename_columns', 'clean_content', 'init_duckdb', 'create_duckdb_table']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0fc0784",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T19:08:07.475122Z",
     "iopub.status.busy": "2023-10-18T19:08:07.474832Z",
     "iopub.status.idle": "2023-10-18T19:08:07.903996Z",
     "shell.execute_reply": "2023-10-18T19:08:07.902641Z"
    },
    "lines_to_next_cell": 1,
    "papermill": {
     "duration": 0.436365,
     "end_time": "2023-10-18T19:08:07.906055",
     "exception": false,
     "start_time": "2023-10-18T19:08:07.469690",
     "status": "completed"
    },
    "tags": [],
    "title": "datadownload.ipynb 1"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "import csv\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import json\n",
    "import duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7645ff0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T19:08:07.914135Z",
     "iopub.status.busy": "2023-10-18T19:08:07.912817Z",
     "iopub.status.idle": "2023-10-18T19:08:07.923241Z",
     "shell.execute_reply": "2023-10-18T19:08:07.921733Z"
    },
    "lines_to_next_cell": 1,
    "papermill": {
     "duration": 0.016107,
     "end_time": "2023-10-18T19:08:07.925080",
     "exception": false,
     "start_time": "2023-10-18T19:08:07.908973",
     "status": "completed"
    },
    "tags": [],
    "title": "datadownload.ipynb 3"
   },
   "outputs": [],
   "source": [
    "def extract_metadata(metadata_url):\n",
    "    \"\"\"\n",
    "    Extracts a list of filenames and urls from Open Cananda metadata url.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    metadata_url : str\n",
    "        Fuel consumption ratings metadata url from Open Canada website.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    english_resources_df : pd.DataFrame\n",
    "        DataFrame of file names and urls for energy consumption ratings.\n",
    "    \"\"\"\n",
    "    try:      \n",
    "        metadata_resp = requests.get(metadata_url)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # If request fails, return an error message and stop.\n",
    "        print(f'Error making url request: {e}')\n",
    "    \n",
    "    try:     \n",
    "        metadata_json = metadata_resp.json()\n",
    "    except json.JSONDecodeError:\n",
    "        # If parsing json fails, return an error message and stop.\n",
    "        print(f'Error: Response is not valid json')\n",
    "        \n",
    "    # Access list of downloadable resources\n",
    "    resources_df = pd.DataFrame(metadata_json['result']['resources'])\n",
    "\n",
    "    # Change language coding and extract English only resources\n",
    "    resources_df['language'] = resources_df['language'].apply(lambda item : item[0])\n",
    "    english_resources_df = resources_df[resources_df['language'] == 'en']\n",
    "    \n",
    "    return english_resources_df[['name', 'url']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f650fd6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T19:08:07.932422Z",
     "iopub.status.busy": "2023-10-18T19:08:07.932130Z",
     "iopub.status.idle": "2023-10-18T19:08:07.939151Z",
     "shell.execute_reply": "2023-10-18T19:08:07.938218Z"
    },
    "lines_to_next_cell": 1,
    "papermill": {
     "duration": 0.013352,
     "end_time": "2023-10-18T19:08:07.940639",
     "exception": false,
     "start_time": "2023-10-18T19:08:07.927287",
     "status": "completed"
    },
    "tags": [],
    "title": "datadownload.ipynb 4"
   },
   "outputs": [],
   "source": [
    "def extract_raw_data(url, file_name):\n",
    "    \"\"\"\n",
    "    Extract raw data from a URL\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        URL to extract data from\n",
    "        \n",
    "    file_name : str or Path object\n",
    "        file name for raw data dump\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Request data from url\n",
    "        response = requests.get(url)\n",
    "        content_type = response.headers['content-type']\n",
    "        response_text = response.text\n",
    "        print(f'Response status: {response.status_code}\\nContent Type: {content_type}')\n",
    "\n",
    "        # Save request content to csv file\n",
    "        with open(file_name, mode='w', newline='') as csvfile:\n",
    "            csvfile.write(response_text)\n",
    "\n",
    "        print(f'csv file: {file_name} saved')\n",
    "        \n",
    "    # Catch errors    \n",
    "    except requests.exceptions.HTTPError as err_h:\n",
    "        print(f'HTTP error occured:{err_h}')\n",
    "    except requests.exceptions.ConnectionError as err_c:\n",
    "        print(f'Error connecting:{err_c}')\n",
    "    except requests.exceptions.Timeout as err_t:\n",
    "        print(f'Timeout Error:{err_t}')\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        print(f'There was an unknown error:{err}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29f31df0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T19:08:07.949298Z",
     "iopub.status.busy": "2023-10-18T19:08:07.947984Z",
     "iopub.status.idle": "2023-10-18T19:08:07.956126Z",
     "shell.execute_reply": "2023-10-18T19:08:07.954765Z"
    },
    "lines_to_next_cell": 1,
    "papermill": {
     "duration": 0.015176,
     "end_time": "2023-10-18T19:08:07.958019",
     "exception": false,
     "start_time": "2023-10-18T19:08:07.942843",
     "status": "completed"
    },
    "tags": [],
    "title": "datadownload.ipynb 5"
   },
   "outputs": [],
   "source": [
    "def merge_top_two_rows(input_file, output_file):\n",
    "    # Open the input CSV file for reading\n",
    "    with open(input_file, mode='r', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        \n",
    "        # Read the first two rows from the input file\n",
    "        header_row = next(reader)\n",
    "        second_row = next(reader)\n",
    "        \n",
    "        # Merge the two rows into one header\n",
    "        merged_header = [f\"{header_row[i]} {second_row[i]}\" for i in range(len(header_row))]\n",
    "        \n",
    "        # Open the output CSV file for writing\n",
    "        with open(output_file, mode='w', newline='') as output_csvfile:\n",
    "            writer = csv.writer(output_csvfile)\n",
    "            \n",
    "            # Write the merged header to the output file\n",
    "            writer.writerow(merged_header)\n",
    "            \n",
    "            # Copy the rest of the rows from the input file to the output file\n",
    "            for row in reader:\n",
    "                writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7431fc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T19:08:07.967669Z",
     "iopub.status.busy": "2023-10-18T19:08:07.967287Z",
     "iopub.status.idle": "2023-10-18T19:08:07.977994Z",
     "shell.execute_reply": "2023-10-18T19:08:07.976817Z"
    },
    "lines_to_next_cell": 1,
    "papermill": {
     "duration": 0.01867,
     "end_time": "2023-10-18T19:08:07.980039",
     "exception": false,
     "start_time": "2023-10-18T19:08:07.961369",
     "status": "completed"
    },
    "tags": [],
    "title": "datadownload.ipynb 6"
   },
   "outputs": [],
   "source": [
    "def rename_columns(df):\n",
    "    \"\"\"\n",
    "    Removes unwanted DataFrame columns and rows, then cleans and renames column headers.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: DataFrame\n",
    "        DataFrame with columns to clean\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df: DataFrame\n",
    "        DataFrame with cleaned column headers\n",
    "\n",
    "    \"\"\"\n",
    "    # Drop empty columns and rows from the DataFrame\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "    df.dropna(axis=0, thresh=5, inplace=True)\n",
    "\n",
    "    # Remove whitespace, replace spaces with _ and change to lower case\n",
    "    cleaned_cols = (df.columns.str.lower()\n",
    "                    .str.strip()\n",
    "                    .str.replace(' # = high output engine', '')\n",
    "                    .str.replace('*', '')\n",
    "                    .str.replace('  ', ' ')\n",
    "                    .str.replace(' ', '_')\n",
    "                    .str.replace('(', '')\n",
    "                    .str.replace(')', '')\n",
    "                    .str.replace('/', '_')\n",
    "                    .str.replace('fuel_consumption_', '')\n",
    "                    .str.replace('consumption_', '')\n",
    "                    .str.replace('_le_', '_l_')\n",
    "                    .str.replace('city_l_100_km', 'consumption_city_l_100_km')\n",
    "                    .str.replace('comb_l_100_km', 'consumption_comb_l_100_km')\n",
    "                    .str.replace('hwy_l_100_km', 'consumption_hwy_l_100_km')\n",
    "    )\n",
    "\n",
    "    col_mapper = dict(list(zip(df.columns, cleaned_cols))) # build a dictionary to map old column names to new\n",
    "    df.rename(columns=col_mapper, inplace=True)\n",
    "\n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "881547a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T19:08:07.986696Z",
     "iopub.status.busy": "2023-10-18T19:08:07.986375Z",
     "iopub.status.idle": "2023-10-18T19:08:07.993905Z",
     "shell.execute_reply": "2023-10-18T19:08:07.992820Z"
    },
    "lines_to_next_cell": 1,
    "papermill": {
     "duration": 0.013552,
     "end_time": "2023-10-18T19:08:07.995826",
     "exception": false,
     "start_time": "2023-10-18T19:08:07.982274",
     "status": "completed"
    },
    "tags": [],
    "title": "datadownload.ipynb 7"
   },
   "outputs": [],
   "source": [
    "def clean_content(df):\n",
    "    \"\"\"\n",
    "    Clean content of master_df columns\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Should be master_df containing all fuel rating data combined\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set fuel type columns using fuel_dict\n",
    "    df['fuel_type'] = df['fuel_type'].map(fuel_dict)\n",
    "    df['fuel_type_1'] = df['fuel_type_1'].map(fuel_dict)\n",
    "    df['fuel_type_2'] = df['fuel_type_2'].map(fuel_dict)\n",
    "\n",
    "    # Set make, model and vehicle_class to lower case and remove \":\" characters\n",
    "    df['make'] = df['make'].str.lower().str.strip()\n",
    "    df['model'] = df['model'].str.lower().str.strip()\n",
    "    df['vehicle_class'] = df['vehicle_class'].str.lower().str.strip()\n",
    "    df['vehicle_class'] = df['vehicle_class'].str.replace(\":\", \"-\")\n",
    "\n",
    "    # Set make, model and vehicle_class to category columns\n",
    "    df['make'] = df['make'].astype('category')\n",
    "    df['model'] = df['model'].astype('category')\n",
    "    df['vehicle_class'] = df['vehicle_class'].astype('category')\n",
    "\n",
    "    # Split transmission column into transmission type and number of gears\n",
    "    df = df.join(df['transmission'].str.split(r\"(\\d+)\", expand=True)\n",
    "                          .drop(columns=[2])\n",
    "                          .rename(columns={0: 'transmission_type', 1: 'number_of_gears'})\n",
    "    )\n",
    "\n",
    "    df['transmission_type'] = df['transmission_type'].map(transmission_dict)\n",
    "    df.drop(columns='transmission', inplace=True)\n",
    "\n",
    "    df.reset_index()\n",
    "    df['id'] = df.index\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56be933d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T19:08:08.005187Z",
     "iopub.status.busy": "2023-10-18T19:08:08.004892Z",
     "iopub.status.idle": "2023-10-18T19:08:08.010838Z",
     "shell.execute_reply": "2023-10-18T19:08:08.009321Z"
    },
    "lines_to_next_cell": 1,
    "papermill": {
     "duration": 0.013683,
     "end_time": "2023-10-18T19:08:08.012619",
     "exception": false,
     "start_time": "2023-10-18T19:08:07.998936",
     "status": "completed"
    },
    "tags": [],
    "title": "datadownload.ipynb 8"
   },
   "outputs": [],
   "source": [
    "def init_duckdb(db_file_path, tables):\n",
    "    \"\"\"\n",
    "    Initialize a DuckDB data base and create tables for each DataFrame\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    db_file_path : str\n",
    "        Path to the DuckDB database file\n",
    "    tables : list\n",
    "        Dictionary of table names and references to DataFrames for those tables \n",
    "    \"\"\"\n",
    "    db_connection = duckdb.connect(db_file_path)\n",
    "    for key, value in tables.items():\n",
    "        create_duckdb_table(db_connection, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7176524d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T19:08:08.022922Z",
     "iopub.status.busy": "2023-10-18T19:08:08.021659Z",
     "iopub.status.idle": "2023-10-18T19:08:08.029366Z",
     "shell.execute_reply": "2023-10-18T19:08:08.027602Z"
    },
    "lines_to_next_cell": 1,
    "papermill": {
     "duration": 0.016441,
     "end_time": "2023-10-18T19:08:08.031556",
     "exception": false,
     "start_time": "2023-10-18T19:08:08.015115",
     "status": "completed"
    },
    "tags": [],
    "title": "datadownload.ipynb 9"
   },
   "outputs": [],
   "source": [
    "def create_duckdb_table(db_connection, table_name, df):\n",
    "    \"\"\"\n",
    "    Create a table in DuckDB\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    db_connection : duckdb.conect\n",
    "        Connection to DuckDB\n",
    "    table_name : str\n",
    "        Name of the table to be created\n",
    "    df : str\n",
    "        Nmae of the DataFrame to be used to create the table.\n",
    "    \"\"\"\n",
    "    db_connection.execute(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "    db_connection.execute(f\"CREATE TABLE {table_name} AS SELECT * FROM {df}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2baf38f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T19:08:08.039625Z",
     "iopub.status.busy": "2023-10-18T19:08:08.039228Z",
     "iopub.status.idle": "2023-10-18T19:08:08.046182Z",
     "shell.execute_reply": "2023-10-18T19:08:08.044904Z"
    },
    "papermill": {
     "duration": 0.01426,
     "end_time": "2023-10-18T19:08:08.048482",
     "exception": false,
     "start_time": "2023-10-18T19:08:08.034222",
     "status": "completed"
    },
    "tags": [],
    "title": "datadownload.ipynb 10"
   },
   "outputs": [],
   "source": [
    "global model_dict\n",
    "global transmission_dict\n",
    "global fuel_dict\n",
    "\n",
    "model_dict = {\n",
    "    \"4wd/4X4\": \"Four-wheel drive\",\n",
    "    \"awd\": \"All-wheel drive\",\n",
    "    \"ffv\": \"Flexible-fuel vehicle\",\n",
    "    \"swb\": \"Short wheelbase\",\n",
    "    \"lwb\": \"Long wheelbase\",\n",
    "    \"ewb\": \"Extended wheelbase\",\n",
    "    \"cng\": \"Compressed natural gas\",\n",
    "    \"ngv\": \"Natural gas vehicle\",\n",
    "    \"#\": \"High output engine that \\\n",
    "            provides more power than the standard \\\n",
    "            engine of the same size\",\n",
    "}\n",
    "\n",
    "transmission_dict = {\n",
    "    \"A\": \"automatic\",\n",
    "    \"AM\": \"automated manual\",\n",
    "    \"AS\": \"automatic with select Shift\",\n",
    "    \"AV\": \"continuously variable\",\n",
    "    \"M\": \"manual\",\n",
    "}\n",
    "\n",
    "fuel_dict = {\n",
    "    \"X\": \"regular gasoline\",\n",
    "    \"Z\": \"premium gasoline\",\n",
    "    \"D\": \"diesel\",\n",
    "    \"E\": \"ethanol (E85)\",\n",
    "    \"N\": \"natural gas\",\n",
    "    \"B\": \"electricity\",\n",
    "    \"B/X\": \"electricity & regular gasoline\",\n",
    "    \"B/Z\": \"electricity & premium gasoline\",\n",
    "    \"B/Z*\": \"electricity & premium gasoline\",\n",
    "    \"B/X*\": \"electricity & regular gasoline\",\n",
    "    \"B\": \"electricity\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb7cf0cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T19:08:08.058170Z",
     "iopub.status.busy": "2023-10-18T19:08:08.056357Z",
     "iopub.status.idle": "2023-10-18T19:08:11.701714Z",
     "shell.execute_reply": "2023-10-18T19:08:11.700865Z"
    },
    "papermill": {
     "duration": 3.652923,
     "end_time": "2023-10-18T19:08:11.703945",
     "exception": false,
     "start_time": "2023-10-18T19:08:08.051022",
     "status": "completed"
    },
    "tags": [],
    "title": "datadownload.ipynb 11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response status: 200\n",
      "Content Type: text/csv\n",
      "csv file: /home/jsh/vehicle-co2/pipeline/data/raw/battery-electric_vehicles_2012-2023_v_2023-10-05.csv saved\n",
      "Response status: 200\n",
      "Content Type: text/csv\n",
      "csv file: /home/jsh/vehicle-co2/pipeline/data/raw/plug-in_hybrid_electric_vehicles_2012-2023_v_2023-10-05.csv saved\n",
      "Response status: 200\n",
      "Content Type: text/csv\n",
      "csv file: /home/jsh/vehicle-co2/pipeline/data/raw/2023_fuel_consumption_ratings_v_2023-08-18.csv saved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response status: 200\n",
      "Content Type: text/csv\n",
      "csv file: /home/jsh/vehicle-co2/pipeline/data/raw/2022_fuel_consumption_ratings_v_2023-08-18.csv saved\n",
      "Response status: 200\n",
      "Content Type: text/csv\n",
      "csv file: /home/jsh/vehicle-co2/pipeline/data/raw/2021_fuel_consumption_ratings_v_2023-02-03.csv saved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response status: 200\n",
      "Content Type: text/csv\n",
      "csv file: /home/jsh/vehicle-co2/pipeline/data/raw/2020_fuel_consumption_ratings_v_2023-02-03.csv saved\n",
      "Response status: 200\n",
      "Content Type: text/csv\n",
      "csv file: /home/jsh/vehicle-co2/pipeline/data/raw/2019_fuel_consumption_ratings_v_2021-09-29.csv saved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response status: 200\n",
      "Content Type: text/csv\n",
      "csv file: /home/jsh/vehicle-co2/pipeline/data/raw/2018_fuel_consumption_ratings_v_2021-09-29.csv saved\n",
      "Response status: 200\n",
      "Content Type: text/csv\n",
      "csv file: /home/jsh/vehicle-co2/pipeline/data/raw/2017_fuel_consumption_ratings_v_2020-03-17.csv saved\n",
      "Response status: 200\n",
      "Content Type: text/csv\n",
      "csv file: /home/jsh/vehicle-co2/pipeline/data/raw/2016_fuel_consumption_ratings_v_2020-03-17.csv saved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response status: 200\n",
      "Content Type: text/csv\n",
      "csv file: /home/jsh/vehicle-co2/pipeline/data/raw/2015_fuel_consumption_ratings_v_2020-03-17.csv saved\n",
      "Response status: 200\n",
      "Content Type: text/csv\n",
      "csv file: /home/jsh/vehicle-co2/pipeline/data/raw/2010-2014_fuel_consumption_ratings_v_2020-03-17.csv saved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response status: 200\n",
      "Content Type: text/csv\n",
      "csv file: /home/jsh/vehicle-co2/pipeline/data/raw/2005-2009_fuel_consumption_ratings_v_2020-01-31.csv saved\n",
      "Response status: 200\n",
      "Content Type: text/csv\n",
      "csv file: /home/jsh/vehicle-co2/pipeline/data/raw/2000-2004_fuel_consumption_ratings.csv saved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response status: 200\n",
      "Content Type: text/csv\n",
      "csv file: /home/jsh/vehicle-co2/pipeline/data/raw/1995-1999_fuel_consumption_ratings.csv saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26874/1295092433.py:46: DtypeWarning: Columns (0,1,2,3,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(merged_header_file_name)\n",
      "/tmp/ipykernel_26874/1295092433.py:46: DtypeWarning: Columns (0,1,2,3,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(merged_header_file_name)\n",
      "/tmp/ipykernel_26874/1295092433.py:46: DtypeWarning: Columns (0,1,2,3,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(merged_header_file_name)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26874/1295092433.py:46: DtypeWarning: Columns (0,1,2,3,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(merged_header_file_name)\n",
      "/tmp/ipykernel_26874/1295092433.py:46: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(merged_header_file_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data downloaded and saved in DuckDB database: /home/jsh/vehicle-co2/pipeline/data/database/car_data.duckdb\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    url = 'https://natural-resources.canada.ca/sites/nrcan/files/oee/files/csv/MY2023%20Fuel%20Consumption%20Ratings.csv'\n",
    "    metadata_url = 'https://open.canada.ca/data/api/action/package_show?id=98f1a129-f628-4ce4-b24d-6f16bf24dd64'\n",
    "    \n",
    "    # Build list of available resources\n",
    "    resources_df = extract_metadata(metadata_url)\n",
    "    \n",
    "    # Remove unwanted old resources\n",
    "    resources_df = resources_df[~resources_df['name'].str.contains('Original')]\n",
    "    \n",
    "    # Build filenames for desired resources and add to resources_df\n",
    "    file_names = (resources_df['name']\n",
    "         .str.replace(' ', '_')\n",
    "         .str.replace('(', 'v_')\n",
    "         .str.replace(')', '')\n",
    "         .str.lower()\n",
    "    )\n",
    "    resources_df.loc[:,'file_name'] = file_names\n",
    "    \n",
    "    # Build raw data file path\n",
    "    path = Path.cwd()\n",
    "    raw_path = path / 'pipeline' / 'data' / 'raw'\n",
    "    merged_header_path = path / 'pipeline' / 'data' / 'merged-headers'\n",
    "    \n",
    "    # Download and save raw data for each resource\n",
    "    for idx, row in resources_df.iterrows():\n",
    "        url = row.iloc[1]\n",
    "        file_name = row.iloc[2]\n",
    "        raw_file_name = raw_path / f'{file_name}.csv'\n",
    "        merged_header_file_name = merged_header_path / f'{file_name}.csv'\n",
    "        extract_raw_data(url, raw_file_name)\n",
    "        merge_top_two_rows(raw_file_name, merged_header_file_name)\n",
    "    \n",
    "    # Start a list of column headers and initiate a master_df\n",
    "    union_of_headers = set()\n",
    "    master_df = pd.DataFrame()\n",
    "    \n",
    "    # Build the master DataFrame\n",
    "    for idx, row in resources_df.iterrows():\n",
    "        # Open each csv file\n",
    "        url = row.iloc[1]\n",
    "        file_name = row.iloc[2]\n",
    "        merged_header_file_name = merged_header_path / f'{file_name}.csv'\n",
    "    \n",
    "        # Rename the columns\n",
    "        df = pd.read_csv(merged_header_file_name)\n",
    "        df = rename_columns(df)\n",
    "    \n",
    "        # Add vehicle type based on file_name\n",
    "        if 'hybrid' in file_name:\n",
    "            df['vehicle_type'] = 'hybrid'\n",
    "        elif 'electric' in file_name and 'hybrid' not in file_name:\n",
    "            df['vehicle_type'] = 'electric'\n",
    "        else:\n",
    "            df['vehicle_type'] = 'fuel-only'\n",
    "            \n",
    "        # Add any missing column headers to master DataFrame columns\n",
    "        union_of_headers = set.union(union_of_headers, set(df.columns))\n",
    "        missing_cols = set(master_df.columns) - union_of_headers\n",
    "        if len(missing_cols) > 0:\n",
    "            for col in missing_cols:\n",
    "                master_df[col] = pd.Series()\n",
    "            \n",
    "        # Concatenate current df with master_df\n",
    "        master_df = pd.concat([master_df, df], ignore_index=True)\n",
    "        \n",
    "    # Clean the master_df\n",
    "    master_df = clean_content(master_df)\n",
    "    \n",
    "    # Create separate fuel, electric, and hybrid DataFrames\n",
    "    electric_df = master_df.loc[master_df['vehicle_type'] == 'electric'].dropna(axis=1, how='all').reset_index()\n",
    "    hybrid_df = master_df.loc[master_df['vehicle_type'] == 'hybrid'].dropna(axis=1, how='all').reset_index()\n",
    "    fuel_df = master_df.loc[master_df['vehicle_type'] == 'fuel-only'].dropna(axis=1, how='all').reset_index()\n",
    "    \n",
    "    # Create dictionary to pass to init database\n",
    "    tables = {'all_vehicles' : 'master_df', 'electric' : 'electric_df', 'hybrid' : 'hybrid_df', 'fuel' : 'fuel_df'}\n",
    "    \n",
    "    # Create directory for DuckDB database \n",
    "    db_path = path / 'pipeline' / 'data' / 'database'\n",
    "    Path(db_path).mkdir(parents=True, exist_ok=True)\n",
    "    # Create file path for DuckDB database\n",
    "    db_file_path = str(db_path / 'car_data.duckdb')\n",
    "\n",
    "    # Create DuckDB database\n",
    "    init_duckdb(db_file_path, tables)\n",
    "\n",
    "    print(f'Data downloaded and saved in DuckDB database: {db_file_path}')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,tags,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "percent"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "papermill": {
   "duration": 5.3124,
   "end_time": "2023-10-18T19:08:11.931793",
   "exception": null,
   "input_path": "/tmp/tmp6zi0g7es.ipynb",
   "output_path": "/home/jsh/vehicle-co2/pipeline/products/datadownload.ipynb",
   "parameters": {
    "product": {
     "data": "/home/jsh/vehicle-co2/pipeline/data/database/car_data.duckdb",
     "nb": "/home/jsh/vehicle-co2/pipeline/products/datadownload.ipynb"
    }
   },
   "start_time": "2023-10-18T19:08:06.619393"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}